# Global Education Dashboard

## Overview
The Global Education Dashboard (GECD) runs two surveys in each country that participates: a public officials survey and a school-level survey. The Public Official (PO) survey interviews civil servants in the ministry of education at various levels (central MinEDU, regional offices, and district offices). The School survey takes averaged measures of school infrastructure, teacher quality, and student test scores at the school level (thus the unit of observation is the individual school). The goal is to tell a story of if/how bureuacratic quality at MinEDU relates to school-level outcomes. This readme file outlines the code, workflow, and main output documents generated by the Bureaucracy Lab. 

### General Notes
As reccomended by DIME, the code is kept seperate from the data and output files. All code can be found on GitHub, and all necessary input and output files can be found in the reproducibility package. The encrypted files are also included in this package. To run the scripts locally, simply change the root file path for the scripts and the data in `main.R` and `MAIN-global-education-dashboard.do` and run these two scripts in order. 

### Geography 
Geography and administrative regions are central to our anlysis. While names for different administrative levels will vary between countries, we refer to the first-level disaggregation below the country as the region, and the second-level disaggregation as the district. The sampling for the two survyes is also geography-dependent: First a subsample of schools is selected at random from a country-wide registry provided by the Ministry of Education. Then, from the districts in which this subsample of schools reside, a subsample of these districts are selected for interviews of public officials and in the Ministry of Education. Therefore, the district-level sampling overlap is theoretically one-way: each public official sampled is in a district with a school sampled, but each school does not necessarily have a public official survey in the same district. Where schools do not have any public officials in the same district, they are dropped from analysis. Please see sampling notes for more details.

### Final Dataset Construction
The goal is to create a dataset with school level observations and associated school survey data, combined with district-level aggregates of bureaucratic indicator from the public officials survey. We also want to minimize the attrition of school-level observations due to the absence of public official data. We create bureaucratic scores by averaging all data from district-level officials; then we merge these averaged scores to schools in the same distrist. This creates the basis for our final anlysis dataset

### Incorporating Outside Data
In order to control for noise and between-country variation, we have also introduced a series of conditional variables (such as population, electrification rate) at the district level, which are also incorproated into the final analysis dataset. More will be described below in Data Sources <br> 

## Data Sources
### Sources
While the primary data source is raw survey data, there are a number of auxililary data sources that each require various degrees of cleaning or harmonizing before incorporating. 

1. Raw Survey Data: for each country, there is raw survey data for the following: <br>
    - School Survey: taken from .Rdata files and stata <br>
    - Public Officials Survey: taken from .Rdata and stata files <br>
2. World Bank Polygon Data: this is used to harmonize gps coordinates and locations between databases. In other words, the World Bank Polygons and associated names are used as the authoritiative determinant of place names and the real-world geometric shapes of those areas. <br>
3. IPUMS outside survey data: IPUMS is a database of national census and survey data that has been harmonized and optimized from cross-country comparability. The IPUMS thus far has been stored as Rdata but data for additional countries will require making additional requests.<br>
4. GDP Data: The GDP data was generated from nightlight algorithms that produce GDP estimates within a certain kilometer radius of the individual schools. The data GDP data is generally included in the Rdata files. <br>
5. School Enrollment Data: The school enrollment data comes in by-country CSVs/Excels to variying levels of tidyness. In basic structure, each lists, theroetically, the total number of students in each school across the country. Some of these rosters have GPS coordinates with each school, which we consider more reliable than survey GPS data, which is prone to innacurate readings or incomplete data. Unfortunately, the school enrollment data must be mapped to WB administrative places. <br>


### Data File Locations 
There are three main places where data files are kept. The first two are encrypted with VeraCrypt (password protected): <br>
1. Complete/Raw: `A:/` "Raw" data from Survey Solutions. This is minially processed at sometimes comes as Rdata or .dta files. This folder also contains school enrollment data and IPUMS data. The code intends this folder to be mounted to VeraCrypt in the `A:/` drive slot. <br>
2. Project_Encrypt: `B:/` Intermediate processed data that is still identifying/needs to be encrypted. Intermediary datasets are in here and well as final Rdata files (which in the project workflow are still identifying.) The code intends this file to be loaded in the `B:/` drive using VeraCrypt.
3. Project Repository: This folder structure is created with DIME's iefieldkit and houses all stata files and documents related for data cleaning in stata.  <br>



## Workflow
### Introduction 
The workflow is a bit odd but makes sense when considering: a) the project began in Stata under the assumption that the primary activity would be survey analysis with minimal cleaning, and b) R has the capability to handle complex geoprocessing much better than Stata. This project very quickly turned into a GIS-heavy project for eocnometrical reasons: geography is important (i.e., knowing exacting where each observation is located in relation to others observations in terms of administrative regions), and so DIME suggested that I begin by creating a "master" dataset that contains only the raw ids, project ids, and geographic data so that I can refer/merge back to this at various points in the repo. The raw data contain gps coordinates, which are used by the simple features package map points in polygons and extract information from the polygons. I use the World Bank subnational boundaries geojson file to obtain the polygons, and this is important as well. Pretty much all the other data, such IPUMS, enorllment rosters, and survey data, use their own administrative schema and naming convensions and untless you have one authoritative source for all this it can get out of control very quickly. Therefore, basically every datapoint has to be mapped to its name and location in the world bank dataset. It's not terribly complicated but all this has to be perfect. <br>

### Order 
#### R 
The main script is `MAIN.R` which includes R-project settings and runs the following scripts: <br>
1. `wb-poly-import.R`: imports the world bank polygons and converts/simplifys to a useable authoritative version (`wb.poly.m`, read:World Bank Polygons, Main). You will need to edit the code to include the future countries in the survey. <br>
2. `mdataset.R`: pulls all survey, enrollment, and GDP data to form main school and public official datasets. Creates: `final_main_data.Rdata`, which is recycled in `recover`. <br>
3. `recover.R`: not all raw data have GPS data, and this essentially excludes them from the anlysis. This script tries to manually recover **school** observations. Starts with `final_main_data.Rdata` and creates `final_main_data_recovered.Rdata`, and `final_main_school_data_mt.dta` <br>
4. `ipums.R`: imports and incorporates IPUMS data, which creates the final step in the cumulative Rdata `final_main_data_ipums.Rdata` and exports these district averages as `school_dist_conditionals.dta`. <br>
5. `enrollment-gps.R`: imports and processes by-district enrollment, producing `enrollment.Rdata` and `by-district-enrollment.dta`. <br>

#### Stata 
The main script is `MAIN-global-education-dashboard.do` which runs scripts under the following groups: <br>
1. Clean: This of scripts imports each country's raw survey data with iecodebook, cleans, constructs, and de-identifies the data. The deidentification script de-identifies by matching survey id codes to the deidenfited/randomized codes in`final_main_school_data_mt.dta` and then drops necessary variables. <br>
2. Agg (or Aggregate): runs a series of scripts to create district level averages for public official indicators of interest. These averages are then mapped onto school-level observations. <br>


#### Other Key Scripts 
All of these scripts can be run in any order once the R and Stata scripts above are run.
1. The `md` folder houses markdown files that display various regressions. <br>
2. Note that, within each folder there is a `global-setup.do` script, but as the Stata project isn't too complicate there aren't many globals that have to change throughout the project, so `clean-global-setup.do` will hold many of your key globals and settings. <br>


## Running the code
The entire repository can be reproduced in a few as two clicks: one for the R portion and one for the Stata portion. I chose to make this a two- or few-click repository in stead of one- because things can get funky when you try to run Stata code from R etc. I'm also much for comfortable doing high-stakes data cleaning and maniuplation in Stata, otherwise I would have done the whole thing in R. The only reason to not translate the Stata portion to R is that there is a key package that is (as of now) only available in R called `iecodebook`, which harmonizes variable labels, names, value labels, etc very quickly and with human-readable documentation. So for now we have, first, the geoprocessing in R, then data cleaning in Stata. 

### R: Geoprocessing and IPUMS 
The first half of the repository is done is R, and can be run with `MAIN.R`, which calls the following scripts in order: `mdataset.R`, `recover.R`, `ipums.R`, and, optionally, `sumstats.R`. Please not the following settings and notes for each of the scripts: <br>
- MAIN.R: You will need to make the following decisions and set the parameters accordingly: <br>
    - If you want to re-process the World Bank World Polygon Shapefiles. This takes a long time computationally (~10 minutes) and *regenerates the randomized geographic variables `g1`, `g2`, and `g3`*, which means that the *entire* rest of the repository will need to be run, including the stata and analysis parts. <br>
        - if *yes*, rerun shapefile regeneration: set `imprtjson == 0` and ensure `savepoly == 1`. It is reccommended you do this the first time you run the repository, then set the settings to `no` as below. <br>
        - if *no*, don't rerun shapefile regeneration: set `imprtjson == 1` and ensure that `savepoly == 1` still. This will skip the ~10 min-long generation of the shapefiles and simply import the R file you saved if you ran under the `yes` parameters. <br>
    - If you want to export/save the files: this is mostly just a saftey mechanism incase you want to test some code without overwriting files. 
        - If *yes*, export and save .Rdata and .dta files: set `export == 1`. Otherwise, <br>
        - If *no*, don't export or save .Rdata and .dta files: set `export == 0` <br>
    - Finally, if you only want to run select scripts, change the following values to: <br>
        - `1` if you want to run the script, and <br>
        - `0` if you don't want to run the scripts, for:: 
```r,
s1 <- 1 # mdataset.R: main dataset import, processing, construction
s2 <- 1 # recover.R:  recovers missing geoinformation to schools with missing gpgs coords
s3 <- 1 # impums.R:   runs IMPUMS data import, processing, matching to WB poly schema
```


## World Bank Shapefile and Polygon Data 
The National-level World Bank Shapefiles are public, but not the subnational files. Those are availabile only on the World Bank Intranet. They are included in the encrypted folder as .geojson files and the Simple Features package can read them, among others. 

### Geoprocessing
I have created a workflow that relies almost entirely on the Simple Features (sf) package, because it can do wonders, and is compatible with most common file types. I suggest you read a bit about the package here: https://r-spatial.github.io/sf/ if you don't know much about it. The only downside of the sf package is that it's somewhat new, and I don't find the available documentation particularly helpful for what we're doing in this repository, which is actually sort of basic in relation to what the package can do. I have not found an equally powerful or comprehensive package in Stata, so the first half of the repository is in R for this main reason. 


## IPUMS data 
Dan has an account with IPUMS which we have used to generate all data extracts. IPUMS has way too much data to just download all of it. Instead, you get IPUMS data by 'shopping' or browsing through their website (https://international.ipums.org/international/). You log in, create a 'cart' or set of countries and variable selections, and then download the associated files. This download instruction page is very helpful (https://international.ipums.org/international/extract_instructions.shtml). Note that you will be able to read the file you download directly from R if you include the .xml and ddi files in the same directory. 

### Maniuplating IPUMS data 
The IPUMS data comes in as "raw", insofar as you get **all** the observations for the criteria you selected, usually at the individual person level. It's not really raw because the actuall raw survey data has been transformed into their categorization schema. The data also includes person and household weights, so make note of these. You should not try to manipulate the data as you normally do with base R or dplyr, etc because IPUMS have produced their own R package for interacting with the data first. To install and look at the introductory package vignette, run

```r, 
install.packages("ipumsr")
vignette("ipums", package = "ipumsr")
```

I highly reccommend you follow the suggested workflow from 
```r,
vignette("ipums-geography", package = "ipumsr")
```

as this describes what to do with NA values, value labels, and the IPUMS geographic data. Note, for example, that NA values are often coded as a factor level (or various factor levels) in IPUMS, and you may not want that. You'll see from my code that I follow the workflow as described in the vignette.

### Incorporating IPUMS data to the World Bank administrative schema by geography. 
Now the only minor issue with the IPUMS data is that they use their own global administrative schema and not that of the World Bank, which is all well and good, except there's no way to automatically match an IPUMS district to a World Bank district, even though most district names and shapes should be roughly the same. The way I join these (in ipums.R) is by use of the Simple Features (sf) package: `st_join()` can connect two features by largest area overlap and, conveniently, the IPUMS package can import shapefiles to interact with the sf package with `read_ipums_sf()`. See the geography vignette for a detailed tour of this. What you end up with in this repo is a key `district.condls` (as R object) or `school_dist_conditionals.dta` (equivalent Stata version) that has links the WB district codes/names with the "matched" IPUMS district and its aggregated indicators by district. This can then be used to match to districts later in the repository.


## School Enrollment Data
The school enrollment data is a bit haphazard: each file contains a list of pretty much all schools in the country and other data, including enrollment data. Sometimes the data is broken down by grade. Sometimes there are GPS coordinates, but other times not. The main use of our school enrollment data is twofold: we want to know how many total students there are within each school we survey and, also, we want to know how many students are in each district, even for district we don't visit. Both of these variables are used to condition the final regrssion (note that, for privacy reasons, the varaible `enrollment` which measures individual school size, is converted into a categorical varible.) <br>

Each of the country-specific enrollment data files are saved either in .csv or .xlsx and are kept in the "Complete/Raw" VeraCrypt file (or `A:/` Drive) under `/countries/enrollment`. The processing of these files is actually quite tedius and is conducted entirely in `enrollment-gps.R` script. The script itself is located in `scripts/mother/utils` as it only needs to be run once each time a new country is added; the resulting 'product' of the script is an Rdata file `main/enrollment.Rdata`, which contains all essential objects, and a .dta file that has district-level averages of enrollment data, called `by-district-enrollment.dta` for incorporation into the Stata workflow later. 


## Main Output Documents 
### Human-readable Documents
The project has several major output documents that are generated by code or by hand to varying extents. <br>

1. Regression Outputs: (generated from `MAIN-md.R`).in `global-education-dashboard/out/html` <br>
2. Histogram Report: `GLOBE_histograms.pdf`, found in `global-education-dashboard/out` <br>
3. Sampling Analyses: `sampling.html` and `sch-asys-extend.html`, found in `global-education-dashboard/out` <br>
4. By-Country In-Depth Descriptive Anlysis: each country file in `analysis-pt-a` folder, located at `global-education-dashboard/out` <br>
5. Monte Carlo Analysis: `monte-carlo-results.xlsx`, in `global-education-dashboard/out` <br>
6. Basic Descriptives and tables: `july-pres-tables.xlsx` in `global-education-dashboard/out` <br>
7. July Joint EDU and BLAB presentation: located in `presentations` folder in `global-education-dashboard/out` <br>


### Computer-readable Documents
The main working dataset is most easily accessible in Stata's .dta format, located at: `~/global-education-dashboard/baseline/DataSets/final/merge_district_tdist.dta`.<br>

Note that this dataset has observations at the school-level, but only keeps schools that are in the same district in which a district-level public officials was interviewed (parameters made for econometric reasons). Each school then has district-level averaged data from public officials associated with it. The data are de-identified, hence my storing the data outside the VeraCrypted folders. <br>

Full de-identified school and public official Stata-readable datasets can be found in this folder: `global-education-dashboard/baseline/DataSets/Deidentified`


